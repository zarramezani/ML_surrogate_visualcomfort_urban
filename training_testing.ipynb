{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de1a41-c9a6-4108-a53c-b1bad4b5be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, Tuple, Any, List\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# LightGBM\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "    _HAS_LGBM = True\n",
    "    _HAS_LGBM_CB = True\n",
    "except Exception:\n",
    "    try:\n",
    "        from lightgbm import LGBMClassifier\n",
    "        _HAS_LGBM = True\n",
    "        _HAS_LGBM_CB = False\n",
    "    except Exception:\n",
    "        _HAS_LGBM = False\n",
    "        _HAS_LGBM_CB = False\n",
    "        LGBMClassifier = None\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    import xgboost as xgb  \n",
    "    from xgboost import XGBClassifier\n",
    "    _HAS_XGB = True\n",
    "except Exception:\n",
    "    XGBClassifier = None\n",
    "    _HAS_XGB = False\n",
    "\n",
    "# SMOTE\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    _HAS_SMOTE = True\n",
    "except Exception:\n",
    "    SMOTE = None\n",
    "    _HAS_SMOTE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87e256-a9c6-472d-95f4-95aabeda6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"dataset.csv\"\n",
    "\n",
    "TARGET_COLS = [\"vp\", \"vsc\", \"vdf\", \"dsh\", \"sa\", \"peak_irr\"]\n",
    "\n",
    "CLASS_BOUNDS = {\n",
    "    \"vp\": (-1, 3),\n",
    "    \"vsc\": (0, 3),\n",
    "    \"vdf\": (0, 2),\n",
    "    \"dsh\": (0, 3),\n",
    "    \"sa\": (0, 3),\n",
    "    \"peak_irr\": (0, 1),\n",
    "}\n",
    "\n",
    "SMOTE_TARGETS = {\"vdf\", \"dsh\", \"vsc\"}\n",
    "\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Boosting hyperparameters\n",
    "LR_BOOST = 0.05\n",
    "DEPTH_BOOST = 10\n",
    "EARLY_STOP_PATIENCE = 60\n",
    "\n",
    "ITER_CAT = 6000\n",
    "N_EST_LGB = 3000\n",
    "N_EST_XGB = 3000\n",
    "\n",
    "ENABLE_LGBM = True\n",
    "ENABLE_XGB = True\n",
    "\n",
    "USE_GPU_CAT = True\n",
    "USE_GPU_XGB = True\n",
    "\n",
    "TARGET_ALPHA = {\n",
    "    \"vdf\": 3.0,\n",
    "    \"dsh\": 2.3,\n",
    "    \"vsc\": 2.7,\n",
    "    \"vp\": 2.5,\n",
    "    \"sa\": 1.5,\n",
    "    \"peak_irr\": 1.0,\n",
    "}\n",
    "\n",
    "MAX_WEIGHT_MULT = 25.0\n",
    "MIN_WEIGHT = 0.1\n",
    "\n",
    "# Thread control (stability / reproducibility)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"4\"\n",
    "\n",
    "\n",
    "def log(msg: str) -> None:\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d505cf-121d-4b20-8352-cf861a0887f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data utilities\n",
    "\n",
    "def clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop rows with missing targets; replace inf; fill numeric NaNs with column median.\"\"\"\n",
    "    df = df.dropna(subset=TARGET_COLS).copy()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[num_cols] = df[num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    df[num_cols] = df[num_cols].fillna(df[num_cols].median(numeric_only=True))\n",
    "    return df\n",
    "\n",
    "\n",
    "def one_hot_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Build X by excluding target columns and one-hot encoding non-numeric columns.\"\"\"\n",
    "    feats = [c for c in df.columns if c not in TARGET_COLS]\n",
    "    X = df[feats].copy()\n",
    "    nonnum = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if nonnum:\n",
    "        X = pd.get_dummies(X, columns=nonnum, drop_first=True)\n",
    "    return X\n",
    "\n",
    "\n",
    "def class_labels_from_cont(Y_cont: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Discretize continuous targets using rounding + clipping to target-specific bounds.\"\"\"\n",
    "    y = np.zeros_like(Y_cont, dtype=int)\n",
    "    for i, t in enumerate(TARGET_COLS):\n",
    "        lo, hi = CLASS_BOUNDS[t]\n",
    "        y[:, i] = np.clip(np.rint(Y_cont[:, i]).astype(int), lo, hi)\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_XY(df: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return X (features), Y_cont (continuous targets), Y_cls (discrete labels in original label space).\"\"\"\n",
    "    X = one_hot_features(df).astype(np.float32)\n",
    "    Y_cont = df[TARGET_COLS].astype(np.float32).values\n",
    "    Y_cls = class_labels_from_cont(Y_cont)\n",
    "    return X, Y_cont, Y_cls\n",
    "\n",
    "\n",
    "def split_indices_random(n: int, test_size: float, random_state: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Random train/test split indices (matches the Methods description).\"\"\"\n",
    "    idx = np.arange(n)\n",
    "    tr, te = train_test_split(idx, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "    return np.array(tr), np.array(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aee6ea-e538-4b25-a4af-089e73926460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label remapping (per target)\n",
    "\n",
    "def make_label_maps() -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For each target with bounds (lo, hi), build mappings:\n",
    "    - to0: original_label -> 0..K-1\n",
    "    - from0: 0..K-1 -> original_label\n",
    "    \"\"\"\n",
    "    maps: Dict[str, Dict[str, Any]] = {}\n",
    "    for t in TARGET_COLS:\n",
    "        lo, hi = CLASS_BOUNDS[t]\n",
    "        labels = list(range(int(lo), int(hi) + 1))\n",
    "        to0 = {lab: i for i, lab in enumerate(labels)}\n",
    "        from0 = {i: lab for i, lab in enumerate(labels)}\n",
    "        maps[t] = {\"to0\": to0, \"from0\": from0, \"K\": len(labels), \"lo\": lo, \"hi\": hi}\n",
    "    return maps\n",
    "\n",
    "\n",
    "LABEL_MAPS = make_label_maps()\n",
    "\n",
    "\n",
    "def remap_y_to_0k(y: np.ndarray, tname: str) -> np.ndarray:\n",
    "    m = LABEL_MAPS[tname][\"to0\"]\n",
    "    return np.array([m[int(v)] for v in y], dtype=int)\n",
    "\n",
    "\n",
    "def unmap_y_from_0k(y0: np.ndarray, tname: str) -> np.ndarray:\n",
    "    m = LABEL_MAPS[tname][\"from0\"]\n",
    "    return np.array([m[int(v)] for v in y0], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d078b72-af5f-4115-8122-6cc135938e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights + SMOTE\n",
    "\n",
    "def per_target_class_weight_dict(y_tr_0k: np.ndarray, tname: str, Kt: int) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Compute class weights in 0..K-1 space, scaled by TARGET_ALPHA[tname].\n",
    "    Returned dict includes all classes 0..Kt-1 (missing classes default to 1.0).\n",
    "    \"\"\"\n",
    "    uniq, cnt = np.unique(y_tr_0k, return_counts=True)\n",
    "    N = len(y_tr_0k)\n",
    "    Kobs = len(uniq)\n",
    "\n",
    "    base = {int(c): (N / (Kobs * cnt[i])) for i, c in enumerate(uniq)}\n",
    "    a = TARGET_ALPHA.get(tname, 1.0)\n",
    "\n",
    "    out = {k: 1.0 for k in range(Kt)}\n",
    "    for c, w in base.items():\n",
    "        out[int(c)] = float(min(MAX_WEIGHT_MULT, max(MIN_WEIGHT, a * w)))\n",
    "    return out\n",
    "\n",
    "\n",
    "def maybe_smote(Xtr: pd.DataFrame, ytr_0k: np.ndarray, tname: str) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"Apply SMOTE only for selected targets and only if imblearn is available.\"\"\"\n",
    "    if tname not in SMOTE_TARGETS:\n",
    "        return Xtr, ytr_0k\n",
    "\n",
    "    if not _HAS_SMOTE:\n",
    "        log(f\"[SMOTE-{tname}] imblearn not available -> skipping SMOTE.\")\n",
    "        return Xtr, ytr_0k\n",
    "\n",
    "    uniq, cnt = np.unique(ytr_0k, return_counts=True)\n",
    "    min_cnt = int(cnt.min())\n",
    "    if min_cnt < 2:\n",
    "        log(f\"[SMOTE-{tname}] minority count too small ({min_cnt}) -> skipping SMOTE.\")\n",
    "        return Xtr, ytr_0k\n",
    "\n",
    "    k = max(1, min(5, min_cnt - 1))\n",
    "    log(f\"[SMOTE-{tname}] applying SMOTE (k_neighbors={k}) ...\")\n",
    "    sm = SMOTE(random_state=RANDOM_STATE, k_neighbors=k)\n",
    "    X_res, y_res = sm.fit_resample(Xtr, ytr_0k)\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1bab9-e0f8-4dbe-809b-0ba32ecf09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics + saving\n",
    "\n",
    "def cls_metrics(Y_true: np.ndarray, Y_pred: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"Per-target accuracy and macro-F1 in the original label space.\"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for i, t in enumerate(TARGET_COLS):\n",
    "        rows.append(\n",
    "            {\n",
    "                \"target\": t,\n",
    "                \"accuracy\": accuracy_score(Y_true[:, i], Y_pred[:, i]),\n",
    "                \"f1_macro\": f1_score(Y_true[:, i], Y_pred[:, i], average=\"macro\", zero_division=0),\n",
    "            }\n",
    "        )\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.loc[len(df)] = {\"target\": \"MEAN\", \"accuracy\": df[\"accuracy\"].mean(), \"f1_macro\": df[\"f1_macro\"].mean()}\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_preds(name: str, Yte: np.ndarray, Ypred: np.ndarray, out_dir: str) -> None:\n",
    "    \"\"\"Save ground truth and predictions (class labels) per target.\"\"\"\n",
    "    out = pd.DataFrame(\n",
    "        {**{f\"{t}_true_cls\": Yte[:, i] for i, t in enumerate(TARGET_COLS)},\n",
    "         **{f\"{t}_pred_cls\": Ypred[:, i] for i, t in enumerate(TARGET_COLS)}}\n",
    "    )\n",
    "    out.to_csv(os.path.join(out_dir, f\"preds_{name}.csv\"), index=False)\n",
    "\n",
    "\n",
    "def save_train_distribution(Ytr: np.ndarray, out_dir: str) -> None:\n",
    "    \"\"\"Save class distribution for the training split in the original label space.\"\"\"\n",
    "    rows = []\n",
    "    for i, t in enumerate(TARGET_COLS):\n",
    "        u, c = np.unique(Ytr[:, i], return_counts=True)\n",
    "        for uu, cc in zip(u, c):\n",
    "            rows.append({\"target\": t, \"class\": int(uu), \"count\": int(cc)})\n",
    "    pd.DataFrame(rows).to_csv(os.path.join(out_dir, \"train_class_distribution.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b0be1-8af3-4a4c-a1aa-0ceb1094fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "def train_linear_baseline(Xtr: pd.DataFrame, Ytr_cont: np.ndarray, Yte_true_cls: np.ndarray, Xte: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"LinearRegression\"\"\"\n",
    "    log(\">>> [LinearRegression] baseline (regression -> discretization)\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    model = Pipeline([(\"scaler\", StandardScaler()), (\"reg\", LinearRegression())])\n",
    "    model.fit(Xtr, Ytr_cont)\n",
    "\n",
    "    y_pred_cont = model.predict(Xte).astype(float)\n",
    "\n",
    "    preds_cls = np.zeros((y_pred_cont.shape[0], len(TARGET_COLS)), dtype=int)\n",
    "    for j, t in enumerate(TARGET_COLS):\n",
    "        lo, hi = CLASS_BOUNDS[t]\n",
    "        preds_cls[:, j] = np.clip(np.rint(y_pred_cont[:, j]), lo, hi).astype(int)\n",
    "\n",
    "    met = cls_metrics(Yte_true_cls, preds_cls)\n",
    "    f1_mean = float(met.query(\"target=='MEAN'\")[\"f1_macro\"].values[0])\n",
    "    log(f\"<<< [LinearRegression] {time.time() - t0:.1f}s | F1(MEAN)={f1_mean:.3f}\")\n",
    "    return {\"name\": \"LinearRegression\", \"y_cls\": preds_cls, \"cls\": met}\n",
    "\n",
    "\n",
    "def train_rf_per_target(Xtr, Ytr, Xte, Yte, out_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"RandomForestClassifier\"\"\"\n",
    "    log(\">>> [RandomForest] per-target\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    preds = np.zeros_like(Yte, dtype=int)\n",
    "\n",
    "    from os import cpu_count\n",
    "    n_jobs = max(1, (cpu_count() or 4) // 2)\n",
    "\n",
    "    for j, t in enumerate(TARGET_COLS):\n",
    "        Kt = LABEL_MAPS[t][\"K\"]\n",
    "        ytr0 = remap_y_to_0k(Ytr[:, j], t)\n",
    "\n",
    "        cw0 = per_target_class_weight_dict(ytr0, t, Kt)\n",
    "\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=800,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=1,\n",
    "            max_features=\"sqrt\",\n",
    "            bootstrap=True,\n",
    "            max_samples=0.9,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=n_jobs,\n",
    "            class_weight=cw0,\n",
    "        )\n",
    "        rf.fit(Xtr, ytr0)\n",
    "        pred0 = rf.predict(Xte).astype(int)\n",
    "        preds[:, j] = unmap_y_from_0k(pred0, t)\n",
    "\n",
    "        # Feature importance\n",
    "        try:\n",
    "            fi = pd.DataFrame({\"feature\": Xtr.columns, \"importance\": rf.feature_importances_}).sort_values(\"importance\", ascending=False)\n",
    "            fi.to_csv(os.path.join(out_dir, f\"feature_importance_rf__{t}.csv\"), index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    met = cls_metrics(Yte, preds)\n",
    "    f1_mean = float(met.query(\"target=='MEAN'\")[\"f1_macro\"].values[0])\n",
    "    log(f\"<<< [RandomForest] {time.time() - t0:.1f}s | F1(MEAN)={f1_mean:.3f}\")\n",
    "    return {\"name\": \"RandomForest\", \"y_cls\": preds, \"cls\": met}\n",
    "\n",
    "\n",
    "def train_cat_per_target(Xtr, Ytr, Xte, Yte, out_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"CatBoostClassifier\"\"\"\n",
    "    log(\">>> [CatBoost] per-target (GPU if available)\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    preds = np.zeros_like(Yte, dtype=int)\n",
    "\n",
    "    base_params = dict(\n",
    "        learning_rate=LR_BOOST,\n",
    "        depth=DEPTH_BOOST,\n",
    "        iterations=ITER_CAT,\n",
    "        random_seed=RANDOM_STATE,\n",
    "        od_type=\"Iter\",\n",
    "        od_wait=EARLY_STOP_PATIENCE,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    for j, t in enumerate(TARGET_COLS):\n",
    "        Kt = LABEL_MAPS[t][\"K\"]\n",
    "        ytr0 = remap_y_to_0k(Ytr[:, j], t)\n",
    "        yte0 = remap_y_to_0k(Yte[:, j], t)\n",
    "\n",
    "        # Targeted SMOTE (train only)\n",
    "        X_tr, y_tr = maybe_smote(Xtr, ytr0, t)\n",
    "\n",
    "        cw0 = per_target_class_weight_dict(y_tr, t, Kt)\n",
    "\n",
    "        weight_list = [cw0.get(k, 1.0) for k in range(Kt)]\n",
    "\n",
    "        params = dict(base_params)\n",
    "        params[\"loss_function\"] = \"Logloss\" if Kt == 2 else \"MultiClass\"\n",
    "\n",
    "        try:\n",
    "            if USE_GPU_CAT:\n",
    "                params.update(dict(task_type=\"GPU\", devices=\"0\"))\n",
    "            clf = CatBoostClassifier(**params, class_weights=weight_list)\n",
    "            clf.fit(X_tr, y_tr, eval_set=(Xte, yte0), verbose=False, use_best_model=True)\n",
    "        except Exception as e:\n",
    "            log(f\"[CatBoost-{t}] GPU failed → CPU fallback. ({e})\")\n",
    "            params.pop(\"task_type\", None)\n",
    "            params.pop(\"devices\", None)\n",
    "            clf = CatBoostClassifier(**params, class_weights=weight_list)\n",
    "            clf.fit(X_tr, y_tr, eval_set=(Xte, yte0), verbose=False, use_best_model=True)\n",
    "\n",
    "        pred0 = clf.predict(Xte).astype(int).reshape(-1)\n",
    "        preds[:, j] = unmap_y_from_0k(pred0, t)\n",
    "\n",
    "        # Save params per target\n",
    "        try:\n",
    "            with open(os.path.join(out_dir, f\"best_params_catboost__{t}.json\"), \"w\") as f:\n",
    "                json.dump(params, f, indent=2)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Feature importance\n",
    "        try:\n",
    "            fi = clf.get_feature_importance()\n",
    "            pd.DataFrame({\"feature\": Xtr.columns, \"importance\": fi}).sort_values(\"importance\", ascending=False).to_csv(\n",
    "                os.path.join(out_dir, f\"feature_importance_catboost__{t}.csv\"), index=False\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    met = cls_metrics(Yte, preds)\n",
    "    f1_mean = float(met.query(\"target=='MEAN'\")[\"f1_macro\"].values[0])\n",
    "    log(f\"<<< [CatBoost] {time.time() - t0:.1f}s | F1(MEAN)={f1_mean:.3f}\")\n",
    "    return {\"name\": \"CatBoost\", \"y_cls\": preds, \"cls\": met}\n",
    "\n",
    "\n",
    "def train_lgb_per_target(Xtr, Ytr, Xte, Yte, out_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"LightGBMClassifier\"\"\"\n",
    "    if not (_HAS_LGBM and ENABLE_LGBM):\n",
    "        log(\"[LightGBM] disabled or unavailable; skipping.\")\n",
    "        return None\n",
    "\n",
    "    log(\">>> [LightGBM] per-target\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    preds = np.zeros_like(Yte, dtype=int)\n",
    "\n",
    "    for j, t in enumerate(TARGET_COLS):\n",
    "        Kt = LABEL_MAPS[t][\"K\"]\n",
    "        ytr0 = remap_y_to_0k(Ytr[:, j], t)\n",
    "        yte0 = remap_y_to_0k(Yte[:, j], t)\n",
    "\n",
    "        X_tr, y_tr = maybe_smote(Xtr, ytr0, t)\n",
    "\n",
    "        cw0 = per_target_class_weight_dict(y_tr, t, Kt)\n",
    "        objective = \"binary\" if Kt == 2 else \"multiclass\"\n",
    "\n",
    "        clf = LGBMClassifier(\n",
    "            n_estimators=N_EST_LGB,\n",
    "            learning_rate=LR_BOOST,\n",
    "            num_leaves=127,\n",
    "            min_data_in_leaf=30,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            objective=objective,\n",
    "            class_weight=cw0,\n",
    "            verbose=-1,\n",
    "        )\n",
    "        if Kt > 2:\n",
    "            clf.set_params(num_class=Kt)\n",
    "\n",
    "        if _HAS_LGBM_CB:\n",
    "            metric = \"binary_logloss\" if Kt == 2 else \"multi_logloss\"\n",
    "            clf.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(Xte, yte0)],\n",
    "                eval_metric=metric,\n",
    "                callbacks=[early_stopping(EARLY_STOP_PATIENCE), log_evaluation(50)],\n",
    "            )\n",
    "        else:\n",
    "            clf.fit(X_tr, y_tr)\n",
    "\n",
    "        pred0 = clf.predict(Xte).astype(int)\n",
    "        preds[:, j] = unmap_y_from_0k(pred0, t)\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(out_dir, f\"best_params_lgbm__{t}.json\"), \"w\") as f:\n",
    "                json.dump(clf.get_params(), f, indent=2)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    met = cls_metrics(Yte, preds)\n",
    "    f1_mean = float(met.query(\"target=='MEAN'\")[\"f1_macro\"].values[0])\n",
    "    log(f\"<<< [LightGBM] {time.time() - t0:.1f}s | F1(MEAN)={f1_mean:.3f}\")\n",
    "    return {\"name\": \"LightGBM\", \"y_cls\": preds, \"cls\": met}\n",
    "\n",
    "\n",
    "def train_xgb_per_target(Xtr, Ytr, Xte, Yte, out_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"XGBoostClassifier\"\"\"\n",
    "    if not (_HAS_XGB and ENABLE_XGB):\n",
    "        log(\"[XGBoost] disabled or unavailable; skipping.\")\n",
    "        return None\n",
    "\n",
    "    log(\">>> [XGBoost] per-target\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    preds = np.zeros_like(Yte, dtype=int)\n",
    "\n",
    "    for j, t in enumerate(TARGET_COLS):\n",
    "        Kt = LABEL_MAPS[t][\"K\"]\n",
    "        ytr0 = remap_y_to_0k(Ytr[:, j], t)\n",
    "        yte0 = remap_y_to_0k(Yte[:, j], t)\n",
    "\n",
    "        X_tr, y_tr = maybe_smote(Xtr, ytr0, t)\n",
    "\n",
    "        is_bin = (Kt == 2)\n",
    "        cw0 = per_target_class_weight_dict(y_tr, t, Kt)\n",
    "        sw = np.array([cw0.get(int(c), 1.0) for c in y_tr], dtype=float)\n",
    "\n",
    "        def _mk(use_gpu: bool) -> XGBClassifier:\n",
    "            params = dict(\n",
    "                n_estimators=N_EST_XGB,\n",
    "                learning_rate=LR_BOOST,\n",
    "                max_depth=DEPTH_BOOST,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.9,\n",
    "                reg_lambda=1.0,\n",
    "                min_child_weight=2.0,\n",
    "                gamma=0.0,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1,\n",
    "                tree_method=(\"gpu_hist\" if use_gpu else \"hist\"),\n",
    "            )\n",
    "            if is_bin:\n",
    "                params.update(dict(objective=\"binary:logistic\", eval_metric=\"logloss\"))\n",
    "            else:\n",
    "                params.update(dict(objective=\"multi:softprob\", num_class=Kt, eval_metric=\"mlogloss\"))\n",
    "            return XGBClassifier(**params)\n",
    "\n",
    "        model = None\n",
    "\n",
    "        for g in ([USE_GPU_XGB, False] if USE_GPU_XGB else [False]):\n",
    "            try:\n",
    "                model = _mk(g)\n",
    "                try:\n",
    "                    from xgboost.callback import EarlyStopping\n",
    "                    cbs = [EarlyStopping(rounds=EARLY_STOP_PATIENCE, save_best=True, maximize=False)]\n",
    "                except Exception:\n",
    "                    cbs = None\n",
    "\n",
    "                fit_kw = dict(sample_weight=sw, eval_set=[(Xte, yte0)])\n",
    "                if cbs is not None:\n",
    "                    fit_kw[\"callbacks\"] = cbs\n",
    "\n",
    "                try:\n",
    "                    model.fit(X_tr, y_tr, **fit_kw)\n",
    "                except TypeError:\n",
    "                    fit_kw.pop(\"callbacks\", None)\n",
    "                    model.fit(X_tr, y_tr, **fit_kw)\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if g:\n",
    "                    log(f\"[XGBoost-{t}] GPU failed → CPU fallback. ({e})\")\n",
    "                model = None\n",
    "\n",
    "        if model is None:\n",
    "            model = _mk(False)\n",
    "            model.fit(X_tr, y_tr, sample_weight=sw)\n",
    "\n",
    "        pred0 = model.predict(Xte).astype(int)\n",
    "        preds[:, j] = unmap_y_from_0k(pred0, t)\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(out_dir, f\"best_params_xgb__{t}.json\"), \"w\") as f:\n",
    "                json.dump(model.get_params(), f, indent=2)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    met = cls_metrics(Yte, preds)\n",
    "    f1_mean = float(met.query(\"target=='MEAN'\")[\"f1_macro\"].values[0])\n",
    "    log(f\"<<< [XGBoost] {time.time() - t0:.1f}s | F1(MEAN)={f1_mean:.3f}\")\n",
    "    return {\"name\": \"XGBoost\", \"y_cls\": preds, \"cls\": met}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b44e40-cb02-4d6d-9b8c-d647113c1288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "def main() -> None:\n",
    "    base = os.path.dirname(CSV_PATH) if os.path.dirname(CSV_PATH) else \".\"\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    out_dir = os.path.join(base, f\"models_classification_clean__{stamp}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    log(f\"Outputs -> {out_dir}\")\n",
    "\n",
    "    log(\"Loading dataset ...\")\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df = clean_df(df)\n",
    "\n",
    "    X, Y_cont, Y_cls = build_XY(df)\n",
    "\n",
    "    # Random split\n",
    "    tr_idx, te_idx = split_indices_random(len(X), TEST_SIZE, RANDOM_STATE)\n",
    "\n",
    "    Xtr, Xte = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "    Ytr_cls, Yte_cls = Y_cls[tr_idx], Y_cls[te_idx]\n",
    "    Ytr_cont = Y_cont[tr_idx]  # for baseline only\n",
    "\n",
    "    log(f\"Shapes: X={X.shape} | train={Xtr.shape} | test={Xte.shape} | targets={len(TARGET_COLS)}\")\n",
    "    log(\"Split strategy: random 80/20 (seed=42)\")\n",
    "\n",
    "    save_train_distribution(Ytr_cls, out_dir)\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    r0 = train_linear_baseline(Xtr, Ytr_cont, Yte_cls, Xte)\n",
    "    results.append(r0)\n",
    "    save_preds(r0[\"name\"], Yte_cls, r0[\"y_cls\"], out_dir)\n",
    "\n",
    "    r2 = train_rf_per_target(Xtr, Ytr_cls, Xte, Yte_cls, out_dir)\n",
    "    results.append(r2)\n",
    "    save_preds(r2[\"name\"], Yte_cls, r2[\"y_cls\"], out_dir)\n",
    "\n",
    "    r3 = train_cat_per_target(Xtr, Ytr_cls, Xte, Yte_cls, out_dir)\n",
    "    results.append(r3)\n",
    "    save_preds(r3[\"name\"], Yte_cls, r3[\"y_cls\"], out_dir)\n",
    "\n",
    "    r4 = train_lgb_per_target(Xtr, Ytr_cls, Xte, Yte_cls, out_dir)\n",
    "    if r4 is not None:\n",
    "        results.append(r4)\n",
    "        save_preds(r4[\"name\"], Yte_cls, r4[\"y_cls\"], out_dir)\n",
    "\n",
    "    r5 = train_xgb_per_target(Xtr, Ytr_cls, Xte, Yte_cls, out_dir)\n",
    "    if r5 is not None:\n",
    "        results.append(r5)\n",
    "        save_preds(r5[\"name\"], Yte_cls, r5[\"y_cls\"], out_dir)\n",
    "\n",
    "    cls_tables = []\n",
    "    for r in results:\n",
    "        dfm = r[\"cls\"].copy()\n",
    "        dfm[\"model\"] = r[\"name\"]\n",
    "        dfm[\"split\"] = \"80_20\"\n",
    "        cls_tables.append(dfm)\n",
    "        dfm.to_csv(os.path.join(out_dir, f\"metrics_{r['name']}.csv\"), index=False)\n",
    "\n",
    "    cls_all = pd.concat(cls_tables, ignore_index=True)\n",
    "    cls_all.to_csv(os.path.join(out_dir, \"classification_all_split.csv\"), index=False)\n",
    "\n",
    "    with pd.ExcelWriter(os.path.join(out_dir, \"results_summary_split.xlsx\")) as xl:\n",
    "        (cls_all[cls_all.target == \"MEAN\"][[\"model\", \"accuracy\", \"f1_macro\", \"split\"]]\n",
    "         .sort_values(\"f1_macro\", ascending=False)\n",
    "         .to_excel(xl, \"cls_MEAN\", index=False))\n",
    "        cls_all.to_excel(xl, \"cls_all\", index=False)\n",
    "\n",
    "    log(\"=== Split 80/20 — MEAN Classification (sorted by Macro-F1) ===\")\n",
    "    print(\n",
    "        cls_all[cls_all.target == \"MEAN\"][[\"model\", \"accuracy\", \"f1_macro\"]]\n",
    "        .sort_values(\"f1_macro\", ascending=False)\n",
    "        .round(4)\n",
    "        .to_string(index=False)\n",
    "    )\n",
    "\n",
    "    meta = dict(\n",
    "        csv_path=CSV_PATH,\n",
    "        timestamp=stamp,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        targets=TARGET_COLS,\n",
    "        class_bounds=CLASS_BOUNDS,\n",
    "        smote_targets=sorted(list(SMOTE_TARGETS)),\n",
    "        target_alpha=TARGET_ALPHA,\n",
    "        booster_params=dict(\n",
    "            learning_rate=LR_BOOST,\n",
    "            depth=DEPTH_BOOST,\n",
    "            early_stop_patience=EARLY_STOP_PATIENCE,\n",
    "            cat_iterations=ITER_CAT,\n",
    "            lgb_estimators=N_EST_LGB,\n",
    "            xgb_estimators=N_EST_XGB,\n",
    "        ),\n",
    "        toggles=dict(\n",
    "            enable_lgbm=ENABLE_LGBM and _HAS_LGBM,\n",
    "            enable_xgb=ENABLE_XGB and _HAS_XGB,\n",
    "            use_gpu_cat=USE_GPU_CAT,\n",
    "            use_gpu_xgb=USE_GPU_XGB,\n",
    "            has_smote=_HAS_SMOTE,\n",
    "        ),\n",
    "        outputs_dir=out_dir,\n",
    "    )\n",
    "    with open(os.path.join(out_dir, \"run_metadata.json\"), \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    log(f\"Saved outputs to: {out_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Orange]",
   "language": "python",
   "name": "conda-env-Orange-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
